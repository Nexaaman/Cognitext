{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "%pwd\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/aman/Desktop/Cognitext'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class PredictionConfig:\n",
    "    model: Path\n",
    "    tokenizer: str\n",
    "    max_token: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Cognitext.utils.common import read_yaml\n",
    "from Cognitext.constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager2:\n",
    "    def __init__(self, config = CONFIG_FILE_PATH, params = PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config)\n",
    "        self.params = read_yaml(params)\n",
    "\n",
    "    def get_prediction_config(self) -> PredictionConfig:\n",
    "        config = self.config.prediction\n",
    "        params = self.params.DataLoaderParams\n",
    "\n",
    "        get_prediction_config = PredictionConfig(\n",
    "            model = config.model,\n",
    "            tokenizer = config.tokenizer,\n",
    "            max_token = params.max_token\n",
    "        )\n",
    "        return get_prediction_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Cognitext.components.ModelTraining import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-09-21 22:36:18,519: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-09-21 22:36:18,591: INFO: common: yaml file: config/config.yaml loaded successfully]\n",
      "[2024-09-21 22:36:18,595: INFO: common: created directory at: artifacts]\n"
     ]
    }
   ],
   "source": [
    "from Cognitext.config.configuration import ConfigurationManager\n",
    "config = ConfigurationManager()\n",
    "get_training_config = config.get_trainer_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prediction:\n",
    "    def __init__(self , config = PredictionConfig):\n",
    "        self.config = config\n",
    "\n",
    "        self.model = Model(get_training_config)\n",
    "        self.tokenizer = tiktoken.get_encoding(self.config.tokenizer)\n",
    "        self.max_token = self.config.max_token\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        model_weights = load_file(self.config.model)  # config.model_path should point to your .safetensors file\n",
    "        self.model.load_state_dict(model_weights)     # Load the weights into the model\n",
    "        \n",
    "        # Move model to the appropriate device\n",
    "        self.model.to(self.device)\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self,text):\n",
    "        encoded = self.tokenizer.encode(text)  # Tokenize the text\n",
    "        input_ids = torch.tensor([encoded])\n",
    "        self.model.eval()  \n",
    "        context_size = self.model.pos_emb.weight.shape[0]\n",
    "\n",
    "        with torch.no_grad():  \n",
    "            for _ in range(self.max_token):\n",
    "                input_cond = input_ids[:, -context_size:]\n",
    "\n",
    "                logits = self.model(input_cond)\n",
    "\n",
    "                logits = logits[:, -1, :]  \n",
    "\n",
    "                probabilities = torch.softmax(logits, dim=-1)\n",
    "                next_token_id = torch.argmax(probabilities, dim=-1, keepdim=True)  \n",
    "\n",
    "                input_ids = torch.cat([input_ids, next_token_id], dim=1)\n",
    "\n",
    "        flat = input_ids.squeeze(0)  \n",
    "        generated_text =  self.tokenizer.decode(flat.tolist())\n",
    "\n",
    "        return generated_text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-09-21 22:39:14,878: INFO: common: yaml file: config/config.yaml loaded successfully]\n",
      "[2024-09-21 22:39:15,011: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-09-21 22:39:15,014: INFO: common: created directory at: artifacts/model_trained]\n",
      "Once upon a time ive got it was the documentary , but i dont hate the room , and the back to get out of the building , i was a mailbox . i was a stretch , and i 's got a grizzly discovery in the other side of the end of the road seemed to the loudspeaker announced his flight and i dont have to the other . i had booked a big bulls-embedded fear doesnt need to be scared shitless of the airline that i had been right ? i was a lot like that i was a few paces and the fringe on the fibrous food group of the whole time explaining that was just\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager2()\n",
    "    prediction_config = config.get_prediction_config()\n",
    "    prediction = Prediction(config=prediction_config)\n",
    "    generated_text = prediction.predict(\"Once upon a time \")\n",
    "    print(generated_text)\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cogni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
